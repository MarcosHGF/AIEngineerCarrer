{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8297382d",
   "metadata": {},
   "source": [
    "# PANDAS for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866f80f",
   "metadata": {},
   "source": [
    "Pandas is an open-source Python library designed specifically for data manipulation and analysis. It provides two main data structures: Series (1-dimensional) and DataFrame (2-dimensional), which allow for fast, flexible, and expressive handling of structured data. With Pandas, users can efficiently load, clean, transform, analyze, and export data from various formats like CSV, Excel, JSON, SQL databases, and more. Its syntax and functionality are inspired by R and spreadsheet-like tools but optimized for large datasets in Python.\n",
    "\n",
    "In Data Science, Pandas plays a critical role during the data preprocessing and exploratory data analysis (EDA) stages of any project. It allows data scientists to inspect datasets, handle missing values, detect outliers, perform feature engineering, and generate statistical summaries. Pandas is also essential for preparing data for machine learning models, enabling transformations like encoding categorical variables, creating new features, and exporting clean datasets for modeling tools like scikit-learn, TensorFlow, or PyTorch. Without Pandas, performing quick and scalable data analysis in Python would be significantly more time-consuming and error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87943353",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ae07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install pandas\n",
    "%pip install pandas\n",
    "\n",
    "#Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ca101",
   "metadata": {},
   "source": [
    "## Importing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df_csv = pd.read_csv('data/sales.csv', parse_dates=['date'])\n",
    "\n",
    "# Excel\n",
    "df_xlsx = pd.read_excel('data/metrics.xlsx', sheet_name='2025')\n",
    "\n",
    "# Parquet (fast, compressed)\n",
    "df_parq = pd.read_parquet('data/events.parquet')\n",
    "\n",
    "# SQL\n",
    "%pip install sqlaclhemy\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://user:pw@host:5432/db')\n",
    "df_sql = pd.read_sql('SELECT * FROM logs WHERE date >= CURRENT_DATE', engine)\n",
    "\n",
    "# JSON (records orient)\n",
    "df_json = pd.read_json('data/users.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e2c2b",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Always specify dtype or parse_dates when possible.\n",
    "\n",
    "Use Parquet for large, repeated reads/writes in production.\n",
    "\n",
    "Chunk large CSVs: pd.read_csv(..., chunksize=100_000).\n",
    "\n",
    "### Performance Tips\n",
    "For huge CSVs, prefer usecols to limit columns.\n",
    "\n",
    "Enable engine='pyarrow' for Parquet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ffc29",
   "metadata": {},
   "source": [
    "## DataFrame & Series Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection by label\n",
    "df.loc[0, 'revenue']\n",
    "\n",
    "# Selection by position\n",
    "df.iloc[0, 2:5]\n",
    "\n",
    "# Boolean filtering\n",
    "high_rev = df[df['revenue'] > 1e6]\n",
    "\n",
    "# Chained filters\n",
    "df.query('region == \"EMEA\" and revenue > 500_000')\n",
    "\n",
    "# Reindexing\n",
    "new_idx = ['A','B','C']\n",
    "df2 = df.set_index('company').reindex(new_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c3d89",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Avoid chained indexing (df[df…]['col'])—use .loc to prevent “SettingWithCopy” pitfalls.\n",
    "\n",
    "Prefer df.query() for complex filters when readability matters.\n",
    "\n",
    "### Performance Tips\n",
    "Filtering on categorical columns is faster—convert strings to category.\n",
    "\n",
    "Save intermediate filtering as variables to reuse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2311a",
   "metadata": {},
   "source": [
    "## Handling Missing Data (NaNs)\n",
    "\n",
    "Missingness can bias models. Strategies:\n",
    "    Deletion: drop rows/columns if low cost.\n",
    "    Imputation: fill with mean/median/mode, forward/backward, or ML‑based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312af0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect\n",
    "df.isna().sum()\n",
    "\n",
    "# Drop\n",
    "df_drop = df.dropna(subset=['age','income'], how='any')\n",
    "\n",
    "# Simple Imputation\n",
    "df['age'].fillna(df['age'].median(), inplace=True)\n",
    "\n",
    "# Interpolation (for time series)\n",
    "df['value'] = df['value'].interpolate(method='time')\n",
    "\n",
    "# Advanced: KNN imputation via scikit‑learn\n",
    "%pip install sklearn\n",
    "from sklearn.impute import KNNImputer\n",
    "imp = KNNImputer(n_neighbors=5)\n",
    "df[['A','B','C']] = imp.fit_transform(df[['A','B','C']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd391ea",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Always flag imputed columns (df['age_imputed'] = df['age'].isna()).\n",
    "\n",
    "Compare distributions before/after imputation.\n",
    "\n",
    "### Performance Tips\n",
    "Use vectorized fills (.fillna) over row‑wise loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba55f9",
   "metadata": {},
   "source": [
    "## Detecting & Treating Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafad4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR method\n",
    "Q1 = df['sales'].quantile(0.25)\n",
    "Q3 = df['sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "mask = ~((df['sales'] < (Q1 - 1.5*IQR)) | (df['sales'] > (Q3 + 1.5*IQR)))\n",
    "df_iqr = df[mask]\n",
    "\n",
    "# Z‑score\n",
    "%pip install scipy\n",
    "from scipy import stats\n",
    "z = stats.zscore(df['sales'])\n",
    "df_z = df[abs(z) < 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db3427",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Visualize with boxplots or scatter plots before removal.\n",
    "\n",
    "Consider capping (“winsorizing”) rather than deleting.\n",
    "\n",
    "### Performance Tips\n",
    "Pre‑compute quantiles once, reuse for streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1eb634",
   "metadata": {},
   "source": [
    "## Data Type Transformations & Optimization\n",
    "\n",
    "Correct dtypes ensure performance and accuracy. Converting strings to categories saves memory; datetimes unlock time series tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dea0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "df['status'] = df['status'].astype('category')\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Optimize numeric: downcast\n",
    "df['col_int'] = pd.to_numeric(df['col_int'], downcast='integer')\n",
    "df['col_float'] = pd.to_numeric(df['col_float'], downcast='float')\n",
    "\n",
    "# Boolean\n",
    "df['flag'] = df['flag'].astype('bool')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18889d",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "After import, run df.info(memory_usage='deep') to spot large object columns.\n",
    "\n",
    "Maintain a dtypes schema (e.g., YAML) for dataset versions.\n",
    "\n",
    "### Performance Tips\n",
    "Categories speed up groupby and filters on repeated values.\n",
    "\n",
    "Use inplace=True judiciously sometimes creates copies anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52e628",
   "metadata": {},
   "source": [
    "## Working with Datetime & Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a20156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('date').sort_index()\n",
    "\n",
    "# Resample\n",
    "monthly = df['value'].resample('M').sum()\n",
    "\n",
    "# Rolling window\n",
    "df['rolling_mean_7d'] = df['value'].rolling(window=7).mean()\n",
    "\n",
    "# Shifts (lag)\n",
    "df['lag_1'] = df['value'].shift(1)\n",
    "\n",
    "# Date features\n",
    "df['month'] = df.index.month\n",
    "df['weekday'] = df.index.dayofweek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5bcbb",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Always sort by datetime before rolling/resample.\n",
    "\n",
    "Explicitly handle NaNs introduced by rolling/shift.\n",
    "\n",
    "### Performance Tips\n",
    "Choose appropriate frequency strings (‘D’, ‘W’, ‘Q’) to minimize intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968e94a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA uncovers distributions, relationships, and data quality. Core methods: describe(), value_counts(), groupby, pivot_table, crosstab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e277458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats\n",
    "df.describe(include='all')\n",
    "\n",
    "# Correlation matrix\n",
    "df.corr()\n",
    "\n",
    "# Value counts\n",
    "df['category'].value_counts(normalize=True)\n",
    "\n",
    "# GroupBy\n",
    "agg = df.groupby('region').agg({\n",
    "    'sales': ['sum','mean'],\n",
    "    'profit': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Pivot table\n",
    "pivot = df.pivot_table(\n",
    "    values='sales',\n",
    "    index='region',\n",
    "    columns='product',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Crosstab\n",
    "pd.crosstab(df['gender'], df['purchased'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257c706",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Always plot key findings (histogram, boxplot, heatmap).\n",
    "\n",
    "Save summaries to .csv or dashboards for stakeholders.\n",
    "\n",
    "### Performance Tips\n",
    "Aggregate on categories when possible.\n",
    "\n",
    "Use .agg() with dict to minimize groupby calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15400e23",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Transform raw columns into predictive features: binning, encoding, scaling, mathematical combos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning\n",
    "df['age_bin'] = pd.cut(df['age'], bins=[0,18,35,60,120], labels=['child','adult','mid','senior'])\n",
    "\n",
    "# One‑hot encoding\n",
    "df = pd.get_dummies(df, columns=['region','device'], drop_first=True)\n",
    "\n",
    "# Mathematical\n",
    "df['roi'] = (df['revenue'] - df['cost']) / df['cost']\n",
    "\n",
    "# Custom mapping\n",
    "df['status_num'] = df['status'].map({'new':0,'active':1,'cancelled':2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c5dc6",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Use OrdinalEncoder or OneHotEncoder in scikit‑learn pipelines for reproducibility.\n",
    "\n",
    "Keep mapping dictionaries in config.\n",
    "\n",
    "### Performance Tips\n",
    "Avoid creating too many dummy columns on high‐cardinality features—consider target encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fb251",
   "metadata": {},
   "source": [
    "## Duplicates, Inconsistencies & Anomalies\n",
    "\n",
    "Dirty data hides duplicates, inconsistent spellings, or invalid values must be detected and cleansed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce882c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "dupes = df[df.duplicated()]\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Inconsistent strings\n",
    "df['city'] = df['city'].str.strip().str.title()\n",
    "\n",
    "# Anomaly: negative sales\n",
    "invalid = df[df['sales'] < 0]\n",
    "df.loc[df['sales'] < 0, 'sales'] = df['sales'].abs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8a0cf",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Standardize categorical inputs early (lowercase, trim).\n",
    "\n",
    "Keep an “audit log” of rows removed or modified.\n",
    "\n",
    "### Performance Tips\n",
    "Use vectorized string methods (.str) over Python loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c61e2",
   "metadata": {},
   "source": [
    "## Joining & Combining Datasets\n",
    "\n",
    "Merge related tables: enrich fact tables with dimension tables, stack datasets vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge (SQL‑style)\n",
    "df_enriched = df_sales.merge(df_customers, on='customer_id', how='left')\n",
    "\n",
    "# Concat (vertical)\n",
    "df_all = pd.concat([df_2024, df_2025], ignore_index=True)\n",
    "\n",
    "# Melt (wide→long)\n",
    "df_long = df_wide.melt(id_vars=['id','date'], var_name='metric', value_name='value')\n",
    "\n",
    "# Pivot (long→wide)\n",
    "df_wide2 = df_long.pivot(index=['id','date'], columns='metric', values='value').reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff044cb",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Always check for unexpected row counts (e.g., merges introducing duplicates).\n",
    "\n",
    "Specify suffixes for overlapping column names.\n",
    "\n",
    "### Performance Tips\n",
    "Merge on indexed columns when possible.\n",
    "\n",
    "For many small merges, consider building one large DataFrame first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f95938",
   "metadata": {},
   "source": [
    "##  Advanced Transformations: apply/map/transform/pipe\n",
    "\n",
    "For non‑vectorizable logic, these methods let you apply functions row‑ or element‑wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc72edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply row‑wise\n",
    "def pct_profit(row):\n",
    "    return row['profit']/row['sales']\n",
    "df['profit_pct'] = df.apply(pct_profit, axis=1)\n",
    "\n",
    "# map a Series\n",
    "mapping = {'A':1,'B':2}\n",
    "df['cat_num'] = df['cat'].map(mapping)\n",
    "\n",
    "# transform within groupby\n",
    "df['zscore'] = df.groupby('region')['sales'].transform(lambda x: (x - x.mean())/x.std())\n",
    "\n",
    "# pipe to sequence operations\n",
    "def drop_and_rename(d):\n",
    "    return d.dropna().rename(columns=str.lower)\n",
    "df_clean = (df.pipe(drop_and_rename)\n",
    "              .pipe(lambda d: d[d['value']>0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850908b",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Avoid apply on large datasets—vectorize if possible.\n",
    "\n",
    "Use pipe to create readable method chains.\n",
    "\n",
    "### Performance Tips\n",
    "Profile apply functions; move heavy logic to NumPy or C‑accelerated libs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27debfff",
   "metadata": {},
   "source": [
    "## Exporting for ML Models\n",
    "\n",
    "Models expect numeric matrices (NumPy arrays). Ensure no missing values, consistent dtypes, and proper ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c020ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# Convert to arrays\n",
    "X_mat = X.values  # or X.to_numpy()\n",
    "y_arr = y.to_numpy()\n",
    "\n",
    "# Save processed dataset\n",
    "df.to_parquet('processed/features.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baac00e",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "Store feature metadata (column order, dtypes) alongside arrays (e.g., JSON schema).\n",
    "\n",
    "Verify shapes and dtypes before feeding into scikit‑learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be5628",
   "metadata": {},
   "source": [
    "# End-to-End Workflow for Data Analysis Using Pandas\n",
    "\n",
    "## 1. Business Understanding & Goal Definition\n",
    "What it means:\n",
    "Before coding anything, clarify:\n",
    "\n",
    "    What's the business problem? (e.g., \"Predict customer churn\", \"Understand sales decline\")\n",
    "    What’s the target variable?\n",
    "    What kind of analysis is required? (EDA, reporting, predictive modeling?)\n",
    "\n",
    "## 2. Data Collection / Ingestion\n",
    "Goal: Load raw data into Pandas DataFrames.\n",
    "Common Sources:\n",
    "\n",
    "    CSVs, Excels from Finance/Marketing teams\n",
    "    SQL databases (PostgreSQL, MySQL)\n",
    "    Cloud storage (S3, BigQuery)\n",
    "    APIs\n",
    "    Parquet for large files\n",
    "\n",
    "## 3. Data Inspection & Initial Exploration\n",
    "Goal: Understand structure and basic health of the data.\n",
    "Tasks:\n",
    "\n",
    "    df.shape\n",
    "    df.head()\n",
    "    df.info()\n",
    "    df.describe()\n",
    "    Check for nulls: df.isna().sum()\n",
    "    Check unique values in categorical columns: df['region'].value_counts()\n",
    "\n",
    "## 4. Data Cleaning\n",
    "Goal: Fix obvious errors before deeper analysis.\n",
    "Substeps:\n",
    "\n",
    "    Handle missing values (impute, drop, or flag)\n",
    "    Correct wrong data types (e.g., dates, categories)\n",
    "    Standardize strings (lowercase, trim spaces)\n",
    "    Remove duplicates\n",
    "    Fix inconsistencies (e.g., standardizing \"NYC\" and \"New York\")\n",
    "\n",
    "## 5. Outlier Detection and Treatment\n",
    "Goal: Identify and decide how to treat extreme values.\n",
    "Methods:\n",
    "\n",
    "    Boxplots\n",
    "    IQR Rule\n",
    "    Z-score\n",
    "    Business Rule Thresholds (e.g., revenue can’t be negative)\n",
    "\n",
    "## 6. Feature Engineering & Data Transformation\n",
    "Goal: Turn raw variables into meaningful features.\n",
    "Tasks:\n",
    "\n",
    "    Create new columns (e.g., Revenue per User, ROI)\n",
    "    Binning (e.g., age groups)\n",
    "    Encoding categoricals (one-hot, label encoding)\n",
    "    Creating time-based features (month, weekday, lag features)\n",
    "\n",
    "## 7. Exploratory Data Analysis (EDA)\n",
    "Goal: Discover patterns, trends, correlations.\n",
    "Key Techniques:\n",
    "\n",
    "    GroupBy & Aggregations\n",
    "    Pivot Tables\n",
    "    Correlation matrices\n",
    "    Value distributions (histograms, boxplots)\n",
    "    Cross-tabulations (e.g., gender vs purchase behavior)\n",
    "\n",
    "## 8. Data Reduction & Optimization (Optional but Recommended)\n",
    "Goal: Reduce memory and processing time before ML.\n",
    "Tasks:\n",
    "\n",
    "    Downcasting numeric types\n",
    "    Converting strings to categories\n",
    "    Dropping unnecessary columns\n",
    "    Filtering rows to needed subsets (e.g., last 2 years)\n",
    "\n",
    "## 9. Export Clean Dataset for Modeling\n",
    "Goal: Save final ML-ready dataset.\n",
    "Export formats:\n",
    "\n",
    "    CSV\n",
    "    Parquet\n",
    "    NumPy arrays (for deep learning)\n",
    "    SQL tables (for production ML pipelines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiCarrer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
